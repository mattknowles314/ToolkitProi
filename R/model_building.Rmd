---
title: "Base model fitting for RTC Data"
author: "220209395"
date: "`r Sys.Date()`"
output:
    pdf_document: default
html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(purrr)
library(openxlsx)
library(naivebayes)


accidents <- read.csv("../Data/dft-road-casualty-statistics-accident-2020.csv")
casualty <- read.csv("../Data/dft-road-casualty-statistics-casualty-2020.csv")
vehicles <- read.csv("../Data/dft-road-casualty-statistics-vehicle-2020.csv")
regions <- read.xlsx("../Data/region_codesV2.xlsx", sheet = "Feuil1", cols = c(1, 2, 3))
```

# Introduction

This report details the baseline process for building a GLM model which is to be used in the MAS61004 group project.

# Data Cleaning

We begin by converting to a binary variable by severity. Accidents with a severity of 3 become a 0, and others are converted to a 1. This is done with a simple ifelse statement, adding a new column to the accidents data.

```{r}
accidents <- accidents %>% 
    mutate(bin_severity = ifelse(.data$accident_severity == 3, 0, 1))
```

We now reduce the size of the dataset to contain only variables we care about, as agreed upon and described in section 3 of the final report. The "sub_X" objects are then combined into one large dataset. Note that on creation of the main dataset (called df) we remove any sex of casualty variables that take on a value of 9 as these are uninformative and so can be excluded. 

```{r}
sub_accidents <- accidents %>% 
    select(
        "accident_index",
        "bin_severity",
        "day_of_week",
        "road_type",
        "speed_limit",
        "time",
        "light_conditions",
        "weather_conditions",
        "local_authority_ons_district",
        "road_surface_conditions"
    )

sub_vehicles <- vehicles %>% 
    select(
        "accident_index",
        "sex_of_driver",
        "age_band_of_driver"
    )

sub_casualties <- casualty %>% 
    select(
        "accident_index",
        "sex_of_casualty",
        "age_band_of_casualty"
    )

df_sub <- merge.data.frame(sub_vehicles, sub_casualties, by = "accident_index")
df_sub_unique <- unique(df_sub)
df <- merge.data.frame(sub_accidents, df_sub_unique, by = "accident_index") %>% 
    filter(sex_of_casualty != 9)
```

Before dealing with other variables in the dataset, we deal with time on its own. The times provided in the dataset are characters, for example of the form "12:14". There are many unique times in the dataset, which would cause issues when fitting models as these would all be treated as individual factors. We therefore split the 24 hour day into 6 chunks of 4 hours, as this dramatically reduces the number of factors, but also gives good information about the relationship between time of day and accident severity. 

The first thing to do is build a function to parse the times by splitting the character before the colon, and converting that to an integer. A series of if-statements are then checked to put the time into the correct category. This isn't the most computationally efficient method, but it does the job. We then apply this function to every time observation to create a new column of data called "time_group", and drop the original time variable from the data so it doesn't cause problems later on.

```{r}
# Split time into 6 chunks of four hours
parse_times <- function(time){
    split_time <- stringr::str_split(time, ":", simplify = TRUE)
    hour <- as.integer(split_time[1])
    if (hour < 4) return(1)
    if (hour < 8 ) return(2)
    if (hour < 12) return(3)
    if (hour < 16) return(4)
    ifelse(hour < 20, return(5), return(6))
}

df <- df %>% 
    mutate(time_group = purrr::map_int(time, parse_times)) %>% 
    select(-time)
```

Location is to be included, but in the current state, the *local_authority_ons_district* variable has too many unique values. To account for this, the region codes (those beginning with E0, N, S or W) are matched to a wider local area, such as the West Midlands, Yorkshire and The Humber etc. Across England, Wales, Scotland & Northern Ireland, there are 12 unique regions. Each region is assigned a numerical valuable. This assignment is done alphabetically, so East Midlands is 1, all the way through to Yorkshire and The Humber at 12. With this in mind, we can add region code to the data. The original location data is then dropped from df.

```{r}
# Obtains the numerical value of a region from a given ONS coded
obtain_region <- function(region_code){
    return(regions$Region.Code[which(regions$Code == region_code)])
}

df <- df %>% 
    mutate(region = purrr::map_int(local_authority_ons_district, obtain_region)) %>% 
    select(-local_authority_ons_district)
```

All variables except speed limit need to be factors. To achieve this we mutate across all integer columns and change the type to factor. However, er need speed limit to stay as an integer to make the model adaptable to non-standard speed limits. The "bin_severity" column needs to be a factor, so we do this by calling the as.factor function once more just to ensure it is infact a factor. By viewing the head of the data as a tibble we can check that the columns are of expected type.

```{r}
df <- df %>%
    mutate(across(where(is.integer), as.factor))
df$speed_limit <- as.integer(df$speed_limit) #Speed limit goes back to integer
df$bin_severity <- as.factor(df$bin_severity)
head(tibble(df))
```

Some observations of the data are recorded as -1. We don't want these in the data, so we trim the data down to remove any rows that contain -1 in any of the columns. This helps with run-time of the model as well due to the size of the data before this is done. At this stage we also remove the accident index variable, as it was only needed for combining the three datasets, and isn't needed in fitting the GLM itself.

```{r}
has.neg <- apply(df, 1, function(row) any(row == -1))
df <- df[-which(has.neg), ] %>% 
    select(-accident_index)
```

# Fitting

We fit a logistic regression to the data using the stats::glm function. We specify that the response variable is binomial, and that we wish to use a logit link function. No other interaction terms are included, as this will be built upon later.

```{r}
fit <- glm(
    data = df,
    formula = bin_severity ~ .,
    family = binomial(link = "logit")
)
```

Let us take a look at the summary and plots of this model.

```{r}
summary(fit)
```

```{r}
plot(fit)
```

This histogram shows the distribution of fitted values from the training data.

```{r}
hist(fit$fitted.values)
```

# Cross Validation & Model Refining

We can perform some bootstrapping on the model. Note that since this process can take a long time to run, it is wrapped in an if statement which checks whether or not the RDS file containing the accuracy dataframe exists first before running the bootstrapping process. If you want to rerun the bootstrapping, delete the accuracy.RDS file in the Data directory.

```{r, echo = FALSE, warning=FALSE}
if (!file.exists("../Data/accuracy.RDS")) {
    df$id <- seq(1:dim(df)[1]) # Sets an ID for each observation in df
    nboot <- 300 # Number of bootstrapping iteration
    
    # Create an empty dataset to hold number of correct and incorrect predictions for each iteration
    accuracy <- matrix(NA, nrow = nboot, ncol = 8,
                              dimnames = list(
                                  rep = seq(nboot),
                                  coef = c("GLM Correct Predictions", "GLM Incorrect Predictions", "NB Correct Predictions", "NB Incorrect Predictions", "GLM False Severe", "GLM False Slight",
                                           "NB False Severe", "NB False Slight"))
                              )
    for (i in seq(nboot)) {
        train <- df %>% sample_frac(1, replace = TRUE)
        test <- anti_join(df,train, by = "id") %>% select(-id)
        
        # Fit a glm and nb models to the same data
        sub_fit <- update(fit, train %>%  select(-id))
        sub_nb <- naive_bayes(bin_severity ~., data = train %>% select(-id), laplace = 0.05)
        
        sub_predicts <- predict.glm(
            sub_fit,
            newdata = test,
            type = "response"
        )
        
        predicts_nb <- predict(
            sub_nb, 
            newdata = test,
            type = "class"
        )
        
        predictions_glm <- ifelse(sub_predicts <= 0.5, 0, 1)
        X <- data.frame(test$bin_severity, predictions_glm, predicts_nb)
        X$GLMResult <- ifelse(X$test.bin_severity == X$predictions_glm, 1, 0)
        X$NBResult <- ifelse(X$test.bin_severity == X$predicts_nb, 1, 0)
        
        X$GLMFalseSevere <- ifelse(X$predictions_glm == 1 & X$test.bin_severity == 0, 1, 0)
        X$GLMFalseSlight <- ifelse(X$predictions_glm == 0 & X$test.bin_severity == 1, 1, 0)
        X$NBFalseSevere <- ifelse(X$predicts_nb == 1 & X$test.bin_severity == 0, 1, 0)
        X$NBFalseSlight <- ifelse(X$predicts_nb == 0 & X$test.bin_severity == 1, 1, 0)
        
        result <- c(length(which(X$GLMResult == 1)), length(which(X$GLMResult == 0)), 
                    length(which(X$NBResult == 1)), length(which(X$NBResult == 0)),
                    length(which(X$GLMFalseSevere == 1)), length(which(X$GLMFalseSlight == 1)),
                    length(which(X$NBFalseSevere == 1)), length(which(X$NBFalseSlight == 1)))
        
        accuracy[i, ] <- result
    }
    
    accuracy <- data.frame(accuracy) %>% 
        mutate(GLMPercCorrect = 100 * (GLM.Correct.Predictions/(GLM.Correct.Predictions + GLM.Incorrect.Predictions))) %>% 
        mutate(NBPercCorrect = 100 * (NB.Correct.Predictions/(NB.Correct.Predictions + NB.Incorrect.Predictions))) %>% 
        mutate(GLMPercFalseSevere = (GLM.False.Severe/GLM.False.Severe + GLM.False.Slight)) %>% 
        mutate(GLMPercFalseSlight = (GLM.False.Slight/GLM.False.Severe + GLM.False.Slight)) %>% 
        mutate(NBPercFalseSevere = (NB.False.Severe/NB.False.Severe + NB.False.Slight)) %>% 
        mutate(NBPercFalseSlight = (NB.False.Slight/NB.False.Severe + NB.False.Slight)) 
    
    saveRDS(accuracy, file = "../Data/accuracy.RDS")
} else {
    accuracy <- readRDS("../Data/accuracy.RDS")
}

```

We can look at several useful plots to see how our model compares against a Naive-Bayes classifier.

```{r}
boxplot(accuracy$GLMPercCorrect, accuracy$NBPercCorrect,
        names = c("GLM", "Naiive Bayes"),
        ylab = "Percentage Correct",
        main = "Percentage of Correctly Predicted Accident Severities")
```

The plot below presents the proportion false predictions in each category. So a case when the model predicted severe but the accident was slight or visa-versa.

```{r}
boxplot(accuracy$GLM.False.Severe, accuracy$GLM.False.Slight,
        accuracy$NB.False.Severe, accuracy$NB.False.Slight,
        names = c("GLM F. Severe", "GLM F. Slight",
                 "NB F. Severe", "NB F. Slight"),
        main = "Proportion of misclassifications"
)
```

Interestingly, the GLM model had a lower proportion of false severes than Naive-Bayes, but a higher proportion of false slight accidents than Naive-Bayes.

# Speed Limit and Road Type analysis

We now present a slightly more specific model than the one we have just seen. We include an interaction term between speed limit and road type, allowing us to assess whether or not speed limits on a given road type are appropriate based on the contribution they make towards accident severity.

```{r}
fit_rt_sl <- glm(
    data = df %>% select(-region),
    formula = bin_severity ~ . + speed_limit*road_type,
    family = binomial(link = "logit")
)
```

We can view the summary of this model as before.

```{r}
summary(fit_rt_sl)
```

Notice the coefficients for road type 2 and 6 (one way street, single carriageway respectively) are on the same order as speed limit overall. This would suggest that perhaps a lower speed limit would be appropriate on thse particular road types. 
