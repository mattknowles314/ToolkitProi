---
title: "Preliminary Analysis of RTC Data"
author: "Matthew Knowles"
date: "`r Sys.Date()`"
output:
    pdf_document: default
html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(purrr)
set.seed(5)
accidents <- read.csv("../Data/dft-road-casualty-statistics-accident-2020.csv")
casualty <- read.csv("../Data/dft-road-casualty-statistics-casualty-2020.csv")
vehicles <- read.csv("../Data/dft-road-casualty-statistics-vehicle-2020.csv")
```

# 1: Data Cleaning

We begin by converting to a binary variable by severity. Accidents with a severity of 1 stay as such, and others are converted to a 0. 

```{r}
accidents <- accidents %>% 
    mutate(bin_severity = ifelse(.data$accident_severity == 1, 1, 0))
```

We now reduce the size of the dataset to contain only variables we care about. 

```{r}
sub_accidents <- accidents %>% 
    select(
        "accident_index",
        "bin_severity",
        "day_of_week",
        "road_type",
        "speed_limit",
        "time",
        "light_conditions",
        "weather_conditions",
        "road_surface_conditions"
    )

sub_vehicles <- vehicles %>% 
    select(
        "accident_index",
        "sex_of_driver",
        "age_band_of_driver"
    )

sub_casualties <- casualty %>% 
    select(
        "accident_index",
        "sex_of_casualty",
        "age_band_of_casualty"
    )

df_sub <- merge.data.frame(sub_vehicles, sub_casualties, by = "accident_index")
df_sub_unique <- unique(df_sub)
df <- merge.data.frame(sub_accidents, df_sub_unique, by = "accident_index")
```

Before dealing with other variables in the dataset, we deal with time on its own. The times provided in the dataset are characters, for example of the form "12:14". There are many unique times in the dataset, which would cause issues when fitting models as these would all be treated as individual factors. We therefore split the 24 hour day into 6 chunks of 4 hours, as this dramatically reduces the number of factors, but also gives good information about the relationship between time of day and accident severity. 

The first thing to do is build a function to parse the times by splitting the character before the colon, and converting that to an integer. A series of if-statements are then checked to put the time into the correct category. This isn't the most computationally efficient method, but it does the job. We then apply this function to every time observation to create a new column of data called "time_group", and drop the original time variable from the data so it doesn't cause problems later on.

```{r}
# Split time into 6 chunks of four hours
parse_times <- function(time){
    if (!is.character(time)) {
        stop("Time is not character")
    }

    split_time <- stringr::str_split(time, ":", simplify = TRUE)
    hour <- as.integer(split_time[1])
    if (hour < 4) return(1)
    if (hour < 8 ) return(2)
    if (hour < 12) return(3)
    if (hour < 16) return(4)
    ifelse(hour < 20, return(5), return(6))
}

df <- df %>% 
    mutate(time_group = purrr::map_int(time, parse_times)) %>% 
    select(-time)
```

We want all variables, except speed limit, to be factors. To achieve this we mutate across all integer columns and change the type to factor. However, we need speed limit to stay as an integer to make the model adaptable to non-standard speed limits. The "bin_severity" column needs to be a factor, so we ensure this by calling the as.factor function once more just to ensure it is infact a factor. By viewing the head of the data as a tibble we can check that the columns are of expected type.

```{r}
df <- df %>%
    mutate(across(where(is.integer), as.factor))
df$speed_limit <- as.integer(df$speed_limit) #Speed limit goes back to integer
df$bin_severity <- as.factor(df$bin_severity)
head(tibble(df))
```

Some observations of the data are -1. We don't want these in the data, so we trim the data down to remove any rows that contain -1 in any of the columns. This helps with run-time of the model as well due to the size of the data before this is done. At this stage we also remove the accident index variable, as it was only needed for combining the three datasets, and isn't needed in fitting the GLM itself.

```{r}
has.neg <- apply(df, 1, function(row) any(row == -1))
df <- df[-which(has.neg), ] %>% 
    select(-accident_index)
```

Finally before fitting, split the data into two sets. A training and test set. The training set has been selected to contain a random sample of 80% of the original data, and the other 20% becomes the test set for later on. There is a of maths one could do to identify an optimal training data set size, but in this case we have selected an 80:20 split as a general rule of thumb.

```{r}
df$id <- 1:nrow(df)
df_train <- df %>% sample_frac(0.8)
df_test <- anti_join(df, df_train, by = "id")
```

# 2: Fitting

We fit the glm to the training data. We specify that the response variable is binomial, and that we wish to use a logit link function.

```{r}
fit <- glm(
    data = df_train,
    formula = bin_severity ~ .,
    family = binomial(link = "logit")
)
```

Let us take a look at the summary and plots of this model.

```{r}
summary(fit)
```

```{r}
plot(fit)
```

This histogram shows the distribution of fitted values from the training data.

```{r}
hist(fit$fitted.values)
```

# 3: Predicting Unseen Values

We can use the predict function to see how well the model predicts the response on unseen data.

```{r}
predict_fit <- predict(
    fit,
    newdata = df_test,
    type = "response"
)
```

This histogram shows the distribution of fitted values from the test data.

```{r}
hist(predict_fit)
```

Notice the shape of this histogram and the previous one are identical.
